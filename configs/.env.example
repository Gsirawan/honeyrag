# =============================================================================
# HONEYRAG CONFIGURATION
# =============================================================================
# Copy this to .env and customize as needed
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Configuration (vLLM)
# -----------------------------------------------------------------------------
# Model to use - must be compatible with your VRAM
# Examples: Qwen/Qwen3-8B (16GB+), Qwen/Qwen2.5-7B-Instruct (14GB+)
VLLM_MODEL=Qwen/Qwen3-8B

# GPU memory utilization (0.0-1.0)
# Higher = more VRAM for model, less for KV cache
VLLM_GPU_MEMORY_UTILIZATION=0.8

# Maximum context length
VLLM_MAX_MODEL_LEN=8192

# vLLM server port
VLLM_PORT=8000

# -----------------------------------------------------------------------------
# Embedding Configuration (Ollama)
# -----------------------------------------------------------------------------
# Embedding model - nomic-embed-text is 768 dimensions
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIM=768

# Ollama server port
OLLAMA_PORT=11434

# -----------------------------------------------------------------------------
# LightRAG Configuration
# -----------------------------------------------------------------------------
# Maximum output tokens (must leave room for input within context)
LIGHTRAG_MAX_TOKENS=2048

# LightRAG server port
LIGHTRAG_PORT=9621

# -----------------------------------------------------------------------------
# Agno Agent Configuration
# -----------------------------------------------------------------------------
# Agent server port (Web UI)
AGNO_PORT=8081

# -----------------------------------------------------------------------------
# Hardware Requirements
# -----------------------------------------------------------------------------
# Minimum: 16GB VRAM (RTX 4080, RTX 3090, etc.)
# Recommended: 24GB VRAM for larger models
# 
# To use smaller models (8GB VRAM):
#   VLLM_MODEL=Qwen/Qwen2.5-3B-Instruct
#   VLLM_GPU_MEMORY_UTILIZATION=0.9
#   VLLM_MAX_MODEL_LEN=4096
