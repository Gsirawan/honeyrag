# =============================================================================
# LIGHTRAG SERVICE CONFIG
# =============================================================================
# Auto-generated by HoneyRAG - connects to vLLM + Ollama
# =============================================================================

# LLM Configuration (vLLM with OpenAI-compatible API)
LLM_BINDING=openai
LLM_MODEL=${VLLM_MODEL:-Qwen/Qwen3-8B}
LLM_BINDING_HOST=http://localhost:${VLLM_PORT:-8000}/v1
LLM_BINDING_API_KEY=not-needed

# Token limits (leave room for input within context)
OPENAI_LLM_MAX_TOKENS=${LIGHTRAG_MAX_TOKENS:-2048}
TIMEOUT=150
MAX_ASYNC=4

# Embedding Configuration (Ollama)
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
EMBEDDING_DIM=${EMBEDDING_DIM:-768}
EMBEDDING_BINDING_HOST=http://localhost:${OLLAMA_PORT:-11434}

# Server Configuration
HOST=0.0.0.0
PORT=${LIGHTRAG_PORT:-9621}
WORKERS=2
WORKING_DIR=./rag_storage

# Document Indexing Settings
ENABLE_LLM_CACHE_FOR_EXTRACT=true
MAX_PARALLEL_INSERT=2
